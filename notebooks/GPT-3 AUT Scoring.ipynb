{"cells":[{"cell_type":"markdown","metadata":{"id":"mGr8loDOutvk"},"source":["# Divergent Thinking Scoring with GPT-3\n","\n","<a href=\"https://colab.research.google.com/github/massivetexts/llm_aut_study/blob/main/notebooks/GPT-3 AUT Scoring.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n","\n","This code is for the GPT-3 portion of experiments in Organisciak, P., Acar, S., Dumas, D., & Berthiaume, K. (2022). Beyond Semantic Distance: Automated Scoring of Divergent Thinking Greatly Improves with Large Language Models. http://dx.doi.org/10.13140/RG.2.2.32393.31840.\n","\n","GPT-3 is a type of large-language-model from OpenAI ([Brown et al 2020](https://arxiv.org/abs/2005.14165)). It was not released publicly, and is offered as a service instead. While that presents challenges to what you can do with your trained models and how you can iterate them, there are also some benefits. OpenAI did a good job with the service, and this is the easiest model to re-apply from our paper."]},{"cell_type":"code","execution_count":2,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23945,"status":"ok","timestamp":1663774948774,"user":{"displayName":"Peter Organisciak","userId":"18310606572067872626"},"user_tz":360},"id":"ps-jueehgpwP","outputId":"7c8a285f-70d7-4e4f-bdff-06fa3d4b08fb"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[?25l\r\u001b[K     |███████▌                        | 10 kB 23.2 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 20 kB 21.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 30 kB 16.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 40 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 43 kB 2.0 MB/s \n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","\u001b[K     |████████████████████████████████| 1.8 MB 9.3 MB/s \n","\u001b[K     |████████████████████████████████| 163 kB 45.3 MB/s \n","\u001b[K     |████████████████████████████████| 158 kB 41.8 MB/s \n","\u001b[K     |████████████████████████████████| 181 kB 46.8 MB/s \n","\u001b[K     |████████████████████████████████| 63 kB 2.1 MB/s \n","\u001b[K     |████████████████████████████████| 157 kB 57.2 MB/s \n","\u001b[K     |████████████████████████████████| 157 kB 81.2 MB/s \n","\u001b[K     |████████████████████████████████| 157 kB 60.7 MB/s \n","\u001b[K     |████████████████████████████████| 157 kB 40.4 MB/s \n","\u001b[K     |████████████████████████████████| 157 kB 78.7 MB/s \n","\u001b[K     |████████████████████████████████| 157 kB 68.5 MB/s \n","\u001b[K     |████████████████████████████████| 157 kB 58.0 MB/s \n","\u001b[K     |████████████████████████████████| 156 kB 64.8 MB/s \n","\u001b[?25h  Building wheel for openai (PEP 517) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["#@title Prep, Installs, imports\n","!pip -qq install openai wandb\n","import openai\n","import time\n","from openai.wandb_logger import WandbLogger\n","from pathlib import Path\n","import numpy as np\n","import json\n","from tqdm.auto import tqdm\n","import pandas as pd\n","from scipy.spatial.distance import cosine\n","tqdm.pandas()\n","\n","#@markdown Point to a text file with your OpenAI key. GPT-3 is a hosted service and fine-tuning/hosting of models is done on their servers.\n","openai.api_key_path = '/content/drive/MyDrive/keys/openaikey.txt' #@param {type:'string'}"]},{"cell_type":"code","execution_count":3,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6492,"status":"ok","timestamp":1663775934102,"user":{"displayName":"Peter Organisciak","userId":"18310606572067872626"},"user_tz":360},"id":"q9fORklq7wtV","outputId":"6c3d94c7-b5d4-483e-e4d5-df2410a5e445"},"outputs":[{"name":"stdout","output_type":"stream","text":["GT options ['gt_main.tar.gz', 'gt_bypart3.tar.gz', 'gt_byprompt4.tar.gz', 'gt_byparticipant.tar.gz', 'gt_byprompt.tar.gz', 'all.tar.gz', 'gt_main2.tar.gz', 'gt_main_std.tar.gz', 'gt_alltests1.tar.gz', 'gt_alltests2.tar.gz']\n","Data decompressed to data/gt_alltests2\n","['group2', 'group3', 'val', 'group1']\n"]}],"source":["#@markdown Working directory.\n","base_dir = Path('drive/MyDrive/Grants/MOTES/') #@param { type: 'raw' }\n","#@markdown Where Ground truth data is held (from [Process AUT GT.ipynb](https://colab.research.google.com/github/massivetexts/llm_aut_study/blob/main/notebook/Process_AUT_GT.ipynb))\n","gt_dir = base_dir / 'Data' / 'aut_ground_truth' #@param { type: 'raw' }\n","print(\"GT options\", [x.name for x in gt_dir.glob('*tar.gz')])\n","#@markdown Name of the ground truth tar.gz file\n","data_subdir = \"gt_alltests2\" #@param ['gt_main2', 'gt_byparticipant', 'gt_byprompt', 'all'] {allow-input: true}\n","#@markdown Where outputs are saved\n","evaldir = base_dir / 'Data' / 'evaluation' / data_subdir #@param { type: 'raw' }\n","evaldir.mkdir(exist_ok=True)\n","\n","# copy locally and unzip\n","!cp \"{gt_dir}/{data_subdir}.tar.gz\" .\n","!rm -rf data\n","!tar -xf {data_subdir}.tar.gz\n","data_dir = Path(f\"data/{data_subdir}\")\n","print(\"Data decompressed to\", data_dir)\n","\n","# this is unsupervised, so go straight to test (unless this is a special case)\n","splits = [x.name for x in data_dir.iterdir() if x.is_dir()]\n","print(splits)\n","if 'test' in splits:\n","    testdata = pd.DataFrame([pd.read_json(x, orient='index')[0] for x in (data_dir / 'test').glob('*json')])\n","    testdata.sample()"]},{"cell_type":"markdown","metadata":{"id":"EBRg4Kyxf_J5"},"source":["# 1. Using Embeddings"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1991,"status":"ok","timestamp":1659379315700,"user":{"displayName":"Peter Organisciak","userId":"18310606572067872626"},"user_tz":360},"id":"y-uNKBlqcxve","outputId":"1322c1a3-65d6-401e-8ec5-6c3dba377a6d"},"outputs":[{"data":{"text/plain":["<__main__.GPTEmbeddingsParquet at 0x7f29fdb4ab10>"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["#@markdown Class def: GPTEmbedding\n","# notes: max is 2048 tokens, replace '\\n' with ' '\n","# https://beta.openai.com/docs/api-reference/embeddings/create\n","#@markdown Init class as `embedder`\n","embedmodel = \"text-similarity-babbage-001\" #@param [\"text-similarity-ada-001\",\"text-similarity-babbage-001\",\"text-similarity-curie-001\",\"text-similarity-davinci-001\"]\n","embedding_dir = base_dir / 'Data' / 'gpt-embeddings' #@param {type:'raw'}\n","\n","#Original JSON model deleted, see pre-deletion in revision 'ConvertedToParquet'\n","class GPTEmbeddingsParquet():\n","    ''' An alternative to GPT Embedding that uses parquet rather than JSON via Pyson '''\n","    PRICING = {\n","        \"text-similarity-ada-001\":\t0.0080,\n","        \"text-similarity-babbage-001\":\t0.0120,\n","        \"text-similarity-curie-001\":\t0.0600,\n","        \"text-similarity-davinci-001\":\t0.6000,\n","    } # per thousand tokens\n","\n","    def __init__(self, model=\"text-similarity-ada-001\", parquet_root='gpt.parquet'):\n","\n","        assert model in self.PRICING.keys()\n","        self.parquet_root = Path(parquet_root)\n","        self.parquet_root.mkdir(exist_ok=True)\n","        self.df = pd.read_parquet(self.parquet_root)\n","        if self.df.empty:\n","            self.df = pd.DataFrame([], columns=['phrase', 'model', 'usage', 'embedding'])\n","        self.model = model\n","        self.total_tokens = 0\n","        self.preload = None\n","        self._buffer = []\n","\n","    def _clean(self, phrase):\n","        return phrase.replace('\\n', '')\n","\n","    def full_embedding(self, phrase, autocommit=True, autocommit_every=400):\n","        ''' Load from parquet or buffer, else make a call to OpenAI (and cache) '''\n","\n","        cleanphrase = self._clean(phrase)\n","\n","        results = self.query(phrase)\n","        if results.empty:\n","            embed = openai.Embedding.create(model=self.model, input=cleanphrase)\n","            input = {\n","                \"phrase\": cleanphrase,\n","                \"model\": self.model,\n","                \"usage\": embed.usage.total_tokens,\n","                \"embedding\": embed.data[0]['embedding']\n","                }\n","            self.total_tokens += embed.usage.total_tokens\n","            self._buffer.append(input)\n","            if autocommit and (len(self._buffer) >= autocommit_every):\n","                self.commit()\n","            return input\n","        else:\n","            return results\n","\n","    def embedding(self, phrase, autocommit=True, autocommit_every=400):\n","        ''' Get just the vector, as an array '''\n","        full = self.full_embedding(phrase,\n","                                   autocommit=autocommit,\n","                                   autocommit_every=autocommit_every)\n","        return np.array(full['embedding'])\n","\n","    def query(self, phrase):\n","        cleanphrase = self._clean(phrase)\n","        results = self.df[(self.df.phrase == phrase) & (self.df.model == self.model)]\n","        if not results.empty:\n","            return results.iloc[0]\n","\n","        results = self._query_buffer(cleanphrase)\n","        if len(results):\n","            return pd.Series(results[0])\n","        else:\n","            return pd.Series(dtype='object') # empty series\n","    \n","    def _query_buffer(self, phrase, early_stop = True):\n","        results = []\n","        for entry in self._buffer:\n","            if (entry['model'] == self.model) and (entry['phrase'] == phrase):\n","                results.append(entry)\n","                if early_stop:\n","                    break\n","        return results\n","\n","    def commit(self):\n","        ''' Write jsonl buffer to parquet '''\n","        if len(self._buffer) == 0:\n","            return\n","        n = len(list(self.parquet_root.glob('*.parquet')))\n","        outname = self.parquet_root / f\"{n+1:04.0f}.parquet\"\n","        \n","        newfile = pd.DataFrame(self._buffer)\n","        newfile.to_parquet(outname, compression='snappy')\n","        self.df = pd.read_parquet(self.parquet_root)\n","        self._buffer = []\n","        self.total_tokens = 0\n","\n","    def getAll(self):\n","        ''' Get all by model '''\n","        return self.db\n","\n","    def get_cost(self, all_runs=False, all_models=False):\n","        ''' Get cost in dollars. If all_runs=True, count up all token use for the model'''\n","        if all_models:\n","            df = pd.DataFrame(self._buffer, columns=['phrase', 'model', 'usage', 'embedding'])\n","            if all_runs:\n","                df = pd.concat([df, self.df])\n","            by_model = df.groupby('model').usage.sum() / 1000 * pd.Series(self.PRICING)\n","            return by_model.fillna(0).round(2)\n","        else:\n","            total_tokens = self.total_tokens\n","            if all_runs:\n","                past_tokens = self.df[self.df.model == self.model].usage.sum()\n","                total_tokens += past_tokens\n","            return total_tokens/1000*self.PRICING[self.model]\n","\n","embedder = GPTEmbeddingsParquet(embedmodel, parquet_root=embedding_dir)\n","embedder"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":81,"referenced_widgets":["c94a87cc40e443089b47cc14ccd590db","088c8aa40bb04668ab1c22c7d397484d","22a33af88e794997917bfeabd9268358","ba0385ef83b24e379b3b8f604285ca02","9bccef81c5b84b9fbd2986d8ddd1b5d5","1c87b8ddd3ee46b5a6ad452bb7e9c5b6","ce2fc29454ac4c4aa23120f50adbf96a","57b78239874441b0bde0a1266675be92","41eea4226fd449538f2e8ab7f75ee6ec","ea7edde4398f4f68aec4afc49192e432","325b1133d7e74183a7f998ead3e5a5ca","dd2d24cdf84a46948f541f1bfd3936d2","e7e28c4dd54349ae8bbdb168f1731560","6fc8c4bc63564a68830df17e92adc5fe","e24031fa5df146628d07ed285c84572d","83fb33a63c29406ca99ea77bb8f4852d","9edded9844eb4898a712fa606a6007b9","2204af6101f345f291afaed89dc0b0a6","10877f4be0fe4c1ea04700363b97affe","e3e493cc1289468b9c9d030cf113748c","ba2083e30d24486f8261c28b95ae302e","7597dc5105b84d9cb54d7093660e1b65"]},"id":"WzOo8o0z_6YD","outputId":"f8efb252-64ea-408f-ec43-5216903f1300"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c94a87cc40e443089b47cc14ccd590db","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/21 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dd2d24cdf84a46948f541f1bfd3936d2","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3030 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["prompt_emb = testdata.drop_duplicates('prompt')[['prompt', 'question']]\n","prompt_emb['pemb'] = prompt_emb.prompt.progress_apply(lambda x: embedder.embedding(x))\n","#prompt_emb['qemb'] = prompt_emb.question.progress_apply(lambda x: embedder.embedding(x))\n","testdata['remb'] = testdata.response.progress_apply(lambda x: embedder.embedding(x))\n","embedder.commit()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ckc5dl8JTFvY"},"outputs":[],"source":["combined = testdata.merge(prompt_emb)\n","combined['predicted'] = combined.apply(lambda x: cosine(x['pemb'], x['remb']), axis=1)\n","#combined['predicted_against_question'] = combined.apply(lambda x: cosine(x['qemb'], x['remb']), axis=1)\n","#combined['predicted_combined'] = combined[['predicted', 'predicted_against_prompt']].mean(1)\n","combined['src'] = combined['id'].apply(lambda x: x.split('_')[0].split('-')[0])\n","combined['model'] = 'gpt-emb-' + embedder.model\n","output = combined[['id', 'model', 'participant', 'prompt', 'target', 'predicted', 'src']]\n","output.to_csv(evaldir / f'gpt-emb-{embedder.model}.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":249},"executionInfo":{"elapsed":330,"status":"ok","timestamp":1659379776413,"user":{"displayName":"Peter Organisciak","userId":"18310606572067872626"},"user_tz":360},"id":"K3XGmGzICKor","outputId":"3215b76d-6f47-4628-c6cf-977851a1027b"},"outputs":[{"data":{"text/plain":["0.1727"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["By task\n"]},{"data":{"text/plain":["src               \n","snbmo09  predicted    0.1509\n","bs12     predicted    0.2142\n","motesf   predicted    0.2143\n","setal08  predicted    0.2434\n","hmsl     predicted    0.2525\n","snb17    predicted    0.3191\n","betal18  predicted    0.3204\n","motesp   predicted    0.3397\n","dod20    predicted    0.3517\n","Name: target, dtype: float64"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["display(combined.corr().loc['predicted', 'target'].round(4))\n","print('By task')\n","combined.groupby('src').corr().round(4)['target'].loc[(slice(None), 'predicted'),].sort_values()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":312,"status":"ok","timestamp":1659379789421,"user":{"displayName":"Peter Organisciak","userId":"18310606572067872626"},"user_tz":360},"id":"yqpZKFNcVrpV","outputId":"d8728ad2-d9f1-4bf6-aa79-42c0acff3441"},"outputs":[{"data":{"text/plain":["prompt               \n","spoon       predicted    0.1113\n","brick       predicted    0.2048\n","hat         predicted    0.2360\n","lightbulb   predicted    0.2640\n","knife       predicted    0.2789\n","box         predicted    0.2801\n","tire        predicted    0.2801\n","book        predicted    0.2809\n","backpack    predicted    0.2847\n","table       predicted    0.2959\n","rope        predicted    0.2987\n","bottle      predicted    0.3257\n","sock        predicted    0.3622\n","paperclip   predicted    0.3712\n","pants       predicted    0.3835\n","shovel      predicted    0.4150\n","fork        predicted    0.4215\n","pencil      predicted    0.4649\n","ball        predicted    0.4652\n","toothbrush  predicted    0.5528\n","shoe        predicted    0.6448\n","Name: target, dtype: float64"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["combined.groupby('prompt').corr().round(4)['target'].loc[(slice(None), 'predicted'),].sort_values()"]},{"cell_type":"markdown","metadata":{"id":"r0aJav0JXR6V"},"source":["## API Costs\n","\n","- `ada`: $`0.07`"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1093,"status":"ok","timestamp":1659379810871,"user":{"displayName":"Peter Organisciak","userId":"18310606572067872626"},"user_tz":360},"id":"vae3FjVcB3Ix","outputId":"5c8d0073-25ca-45bf-ed91-8170aa6a7a9a"},"outputs":[{"data":{"text/plain":["text-similarity-ada-001        0.55\n","text-similarity-babbage-001    0.73\n","text-similarity-curie-001      0.00\n","text-similarity-davinci-001    0.00\n","dtype: float64"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["embedder.get_cost(all_runs=True, all_models=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":288,"status":"ok","timestamp":1659379814692,"user":{"displayName":"Peter Organisciak","userId":"18310606572067872626"},"user_tz":360},"id":"8qStEkJn1XHd","outputId":"dd019525-6e88-404d-dbc6-90574e8f8a80"},"outputs":[{"data":{"text/plain":["usage    61244\n","dtype: int64"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["embedder.df[embedder.df.model.str.contains('babbage')][['usage']].sum()"]},{"cell_type":"markdown","metadata":{"id":"9Gzwi1d3X46O"},"source":["# 2. Fine-tuning"]},{"cell_type":"markdown","metadata":{"id":"POOXz71LTHaI"},"source":["## Fine-tune training"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"w5Xr8cKZ8TcL"},"outputs":[],"source":["#@title Prep data\n","#@markdown completions are multiplied by 10 and rounded\n","all_data = []\n","for split in splits:\n","    if (split == 'val') and ('byprompt' in data_subdir):\n","        continue\n","    print(f'preparing {split} data')\n","    df = pd.DataFrame([pd.read_json(x, orient='index')[0] for x in (data_dir / split).glob('*json')])\n","    df['split'] = split\n","    all_data.append(df)\n","all_data = pd.concat(all_data)\n","\n","def gt_preparation(df):\n","    df = df[~df.target.isna()]\n","    df['response'] = df.response.str.replace('\\n', ' ')\n","    df['completion'] = df.target.apply(lambda x: f' {int(x*10)}')\n","\n","    if 'type' not in df.columns:\n","        # this is the functionality for the first LLM paper, which was AUT only and did not include a 'type' column\n","        df['gptprompt'] = df.apply(lambda x: f\"AUT Prompt:{x['prompt']}\\nResponse:{x['response']}\\nScore:\\n\", 1)\n","    else:\n","        # construct prompts for everything\n","        match = df['type'] == 'uses'\n","        df.loc[match, 'gptprompt'] = df.loc[match].apply(lambda x: f\"DT Uses Prompt:{x['prompt']}\\nResponse:{x['response']}\\nScore:\\n\", 1)\n","\n","        match = (df['type'] == 'instances')\n","        df.loc[match, 'gptprompt'] = df.loc[match].apply(lambda x: f\"DT Instances Prompt:{x['prompt']}\\nResponse:{x['response']}\\nScore:\\n\", 1)\n","\n","        match = (df['type'] == 'completion')\n","        df.loc[match, 'simplequestion'] = df.loc[match, 'question'].str.replace(\"Complete.*?\\: \\\"(.*?)\\.\\.\\.\\\".*\", \"\\\\1\", regex=True).tolist()\n","        df.loc[match, 'gptprompt'] = df.loc[match].apply(lambda x: f\"DT Completion Prompt:{x['simplequestion']}\\nResponse:{x['response']}\\nScore:\\n\", 1)\n","\n","        match = (df['type'] == 'consequences')\n","        df.loc[match, 'gptprompt'] = df.loc[match].apply(lambda x: f\"DT Consequences Prompt:if {x['question'].split('consequence if')[1].replace('?', '').strip()}\\nResponse:{x['response']}\\nScore:\\n\", 1)\n","\n","    return df\n","\n","all_data = gt_preparation(all_data)"]},{"cell_type":"code","execution_count":316,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":299,"status":"ok","timestamp":1663772035782,"user":{"displayName":"Peter Organisciak","userId":"18310606572067872626"},"user_tz":360},"id":"U9IgYmGYFKdi","outputId":"8e913814-058a-4647-8bb8-8b648ed30757"},"outputs":[{"name":"stdout","output_type":"stream","text":["saving train data to finetune-gt_alltests2-perm3_prepared_train.jsonl\n","saving val data to finetune-gt_alltests2-perm3_prepared_val.jsonl\n"]}],"source":["#@title Save data\n","#@markdown Partial splits are for training on less data\n","use_partial = False #@param {type:'boolean'}\n","partial_portions = [0.01, 0.05, 0.1, 0.2, 0.4, 0.6, 0.8, 1.0] #@param {type:\"raw\"}\n","\n","#@markdown The naming suffix can help differentiate custom tweaks to the data. Make it blank if you don't need it (in most cases, you'll want it blank).\n","naming_suffix = '' #@param {type:'string'}\n","\n","#@markdown If using atypical names for the dataset (e.g. group1, group2, etc.), describe here,\n","#@markdown else make it `train = ['train']`, etc. In most cases (such as reproducing results from our paper), you'll want the values to be 'train', 'val', and 'test'.\n","setnames = dict()\n","trainsetnames = ['train'] #@param {type:'raw'}\n","valsetnames= ['val']#@param {type:'raw'}\n","testsetnames = ['test'] #@param {type:'raw'}\n","\n","setnames = dict(train=trainsetnames,\n","                val=valsetnames,\n","                test=testsetnames)\n","\n","# save testset\n","for split in ['train', 'val']:\n","    out = all_data[all_data.split.isin(setnames[split])][['gptprompt', 'completion']]\n","    out.columns = ['prompt', 'completion']\n","    fname = f'finetune-{data_subdir}{naming_suffix}_prepared_{split}.jsonl'\n","    print(f'saving {split} data to {fname}')\n","    out.to_json(fname, orient='records', lines=True)\n","\n","    if use_partial and (split == 'train'):\n","        for prop in partial_portions:\n","            fname = f'finetune-{data_subdir}{naming_suffix}_prepared_{split}-{prop}.jsonl'\n","            print(f'saving {split} data to {fname}')\n","            out.sample(frac=prop, random_state=12345).to_json(fname, orient='records', lines=True)\n","        print(\"Saved partials with the following proportions:\", partial_portions)\n","# use to doublecheck data\n","# don't split since validation already exists"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"hnMlQCoBBLKk"},"outputs":[],"source":["#@markdown Upload training files or retrieve already-trained files from OpenAI\n","sid = dict()\n","\n","def upload_or_load(fname):\n","    existing_files = [file for file in openai.File.list().data if file.filename == fname]\n","    if len(existing_files):\n","        print(f\"Using already uploaded file for {split}. If this was unintended, delete the server one.\")\n","        return existing_files[0].id\n","    else:\n","        print(\"Uploading\", fname)\n","        with open(fname) as f:\n","            results = openai.File.create(file=f, purpose='fine-tune', user_provided_filename=fname)\n","        return results.id\n","\n","for split in ['train', 'val']:\n","    if (split == 'val') and ('byprompt' in data_subdir):\n","        continue\n","    fname = f'finetune-{data_subdir}{naming_suffix}_prepared_{split}.jsonl'\n","    sid[split] = upload_or_load(fname)\n","\n","    if use_partial and (split == 'train'):\n","        for prop in partial_portions:\n","            fname = f'finetune-{data_subdir}{naming_suffix}_prepared_{split}-{prop}.jsonl'\n","            sid[f\"{split}-{prop}\"] = upload_or_load(fname)\n","\n","print(\"files:\", sid.keys())"]},{"cell_type":"markdown","metadata":{"id":"ZfBvanhCD64n"},"source":["This is mainly a note to self, but might be valuable for others applying similar methods. The LLM paper was trained on our data (and other researchers'), but we also needed unbiased scores for our main work on MOTES, meaning the model can't have previously seen the responses that it's scoring. To get around this, we trained multiple models, each with different test/train data, so each response can be scored by a model that hasn't seen it before.\n","\n","e.g. the `alltests2` had three groups of input data, each 32% of the data. purmutation1 is trained on group1+2 (test on group 3), perm2 is trained on group1+3 (test on group 2), and perm3 is trained on group2+3 (test on group 1)."]},{"cell_type":"markdown","metadata":{"id":"9MynaSoQcO-z"},"source":["## Fine-tuning"]},{"cell_type":"code","execution_count":318,"metadata":{"cellView":"form","executionInfo":{"elapsed":291,"status":"ok","timestamp":1663772074304,"user":{"displayName":"Peter Organisciak","userId":"18310606572067872626"},"user_tz":360},"id":"VZLylERs-6R8"},"outputs":[],"source":["finetune_model = \"curie\" #@param [\"ada\", \"babbage\", \"curie\", \"davinci\"]\n","project_name = 'aut-gpt3' #@param {type:'string'}"]},{"cell_type":"code","execution_count":319,"metadata":{"cellView":"form","executionInfo":{"elapsed":304,"status":"ok","timestamp":1663772082713,"user":{"displayName":"Peter Organisciak","userId":"18310606572067872626"},"user_tz":360},"id":"R-9PASxmTZgX"},"outputs":[],"source":["#@markdown ### Start Fine-tuning\n","if use_partial:\n","    print(\"Stopping. Are you sure you don't want to run code from section below, 'Fine-tuning for partial train data'?\")\n","else:\n","    if 'val' in sid:\n","        results = openai.FineTune.create(training_file=sid['train'],\n","                            validation_file=sid['val'],\n","                            model=finetune_model,\n","                            suffix=data_subdir+naming_suffix)\n","    else:\n","        results = openai.FineTune.create(training_file=sid['train'],\n","                            model=finetune_model,\n","                            suffix=data_subdir+naming_suffix)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I4rOV8buvu2w"},"outputs":[],"source":["# check in every few minutes to see training status\n","while True:\n","    status = openai.FineTune.retrieve(id=results.id).status\n","    print(status)\n","    if status not in ['running', 'pending']:\n","        break\n","    time.sleep(5*60)"]},{"cell_type":"markdown","metadata":{"id":"BZ_8JgZY_Ksv"},"source":["### Costs\n","\n","- Davinci: \"Fine-tune costs $35.65\". Curie is 1/10th of the price.\n","\n","*These notes are outdated, from before I grew the dataset:*\n","Fine-tuning was 942,736 tokens according to wandb logs, 620,984 tokens according to the billing interface. On ada: `/1000*0.0004=$0.248`, which is about what the billing interface say. At that rate, Babbage (`0.0006`) would train for `$0.37`, Curie (`0.0030`)for `$1.86`, and Davinci (`0.030`) for `$18.92`.\n","\n","My training+val example n = 8400.\n","\n","Usage cost is 4x per token, though because training had 4 epochs, the cost per item should be about the same. If test data is 15% of the dataset size, just divide the train costs by $5\\frac{2}{3}$.\n","\n","#### Updates with larger dataset\n","\n","- finetuning on gt_main was 1,033,728: 0.41 on ada, 0.61 on babbage, $3.10 on curie, and $31 on davinci."]},{"cell_type":"markdown","metadata":{"id":"IcOEf0nF1NwV"},"source":["## Fine-tuning for partial train data"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"vBwAeHB-1Nd3"},"outputs":[],"source":["finetune_model = \"curie\" #@param [\"ada\", \"babbage\", \"curie\", \"davinci\"]\n","project_name = 'aut-gpt3' #@param {type:'string'}"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8136,"status":"ok","timestamp":1659726118942,"user":{"displayName":"Peter Organisciak","userId":"18310606572067872626"},"user_tz":360},"id":"SyktFpvK1LXO","outputId":"ff858b2b-4f45-4b1e-905e-42f20ea65936"},"outputs":[{"name":"stdout","output_type":"stream","text":["Are you sure you want to start fine-tuning runs for [0.01]? (Y/N)Y\n","0.01\n"]}],"source":["doublecheck = input(f\"Are you sure you want to start fine-tuning runs for {partial_portions}? (Y/N)\")\n","if doublecheck.lower() == 'y':\n","    for prop in partial_portions:\n","        print(prop)\n","        results = openai.FineTune.create(training_file=sid[f'train-{prop}'],\n","                            validation_file=sid['val'],\n","                            model=finetune_model,\n","                            suffix=f'{data_subdir}#{prop}')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":285,"status":"ok","timestamp":1659726121031,"user":{"displayName":"Peter Organisciak","userId":"18310606572067872626"},"user_tz":360},"id":"NHOfN9esUM71","outputId":"4f73d948-abe5-4b5f-ec8a-43b625da69e7"},"outputs":[{"data":{"text/plain":["<FineTune fine-tune id=ft-0lEwbHncradjiReqMlyOYAPp at 0x7ffb1faaccb0> JSON: {\n","  \"created_at\": 1659726118,\n","  \"events\": [\n","    {\n","      \"created_at\": 1659726118,\n","      \"level\": \"info\",\n","      \"message\": \"Created fine-tune: ft-0lEwbHncradjiReqMlyOYAPp\",\n","      \"object\": \"fine-tune-event\"\n","    },\n","    {\n","      \"created_at\": 1659726119,\n","      \"level\": \"info\",\n","      \"message\": \"Fine-tune costs $0.04\",\n","      \"object\": \"fine-tune-event\"\n","    },\n","    {\n","      \"created_at\": 1659726120,\n","      \"level\": \"info\",\n","      \"message\": \"Fine-tune enqueued. Queue number: 0\",\n","      \"object\": \"fine-tune-event\"\n","    }\n","  ],\n","  \"fine_tuned_model\": null,\n","  \"hyperparams\": {\n","    \"batch_size\": 1,\n","    \"learning_rate_multiplier\": 0.1,\n","    \"n_epochs\": 4,\n","    \"prompt_loss_weight\": 0.1\n","  },\n","  \"id\": \"ft-0lEwbHncradjiReqMlyOYAPp\",\n","  \"model\": \"curie\",\n","  \"object\": \"fine-tune\",\n","  \"organization_id\": \"org-48rfjyoSnZfLJWOte33sjuqL\",\n","  \"result_files\": [],\n","  \"status\": \"pending\",\n","  \"training_files\": [\n","    {\n","      \"bytes\": 15458,\n","      \"created_at\": 1659559412,\n","      \"filename\": \"finetune-gt_main2_prepared_train-0.01.jsonl\",\n","      \"id\": \"file-iicfhHW7SJGsCFmtexulvozr\",\n","      \"object\": \"file\",\n","      \"purpose\": \"fine-tune\",\n","      \"status\": \"processed\",\n","      \"status_details\": null\n","    }\n","  ],\n","  \"updated_at\": 1659726120,\n","  \"validation_files\": [\n","    {\n","      \"bytes\": 97015,\n","      \"created_at\": 1659379937,\n","      \"filename\": \"finetune-gt_main2_prepared_val.jsonl\",\n","      \"id\": \"file-vJCdI354B0JF0UnyQuY7elwb\",\n","      \"object\": \"file\",\n","      \"purpose\": \"fine-tune\",\n","      \"status\": \"processed\",\n","      \"status_details\": null\n","    }\n","  ]\n","}"]},"execution_count":43,"metadata":{},"output_type":"execute_result"}],"source":["openai.FineTune.retrieve(results.id)"]},{"cell_type":"markdown","metadata":{"id":"-CgoTSSx6MGK"},"source":["## Test Fine-Tune\n","\n"]},{"cell_type":"code","execution_count":4,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/","height":927},"executionInfo":{"elapsed":904,"status":"ok","timestamp":1663776525063,"user":{"displayName":"Peter Organisciak","userId":"18310606572067872626"},"user_tz":360},"id":"CGfdb_7f_qwI","outputId":"c29a3ae2-a0c9-4118-8672-19c648219a3f"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-211b269b-9d12-4ca4-b686-81957e2ea01a\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th>name</th>\n","      <th>lab</th>\n","      <th>date</th>\n","    </tr>\n","    <tr>\n","      <th>split</th>\n","      <th>size</th>\n","      <th>proportion</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>gt-alltests2-perm1</th>\n","      <th>curie</th>\n","      <th>1.00</th>\n","      <td>curie:ft-massive-texts-lab:gt-alltests2-perm1-...</td>\n","      <td>ft-massive-texts-lab</td>\n","      <td>09-20-23-08-34</td>\n","    </tr>\n","    <tr>\n","      <th>gt-alltests2-perm2</th>\n","      <th>curie</th>\n","      <th>1.00</th>\n","      <td>curie:ft-massive-texts-lab:gt-alltests2-perm2-...</td>\n","      <td>ft-massive-texts-lab</td>\n","      <td>09-21-15-22-40</td>\n","    </tr>\n","    <tr>\n","      <th>gt-alltests2-perm3</th>\n","      <th>curie</th>\n","      <th>1.00</th>\n","      <td>curie:ft-massive-texts-lab:gt-alltests2-perm3-...</td>\n","      <td>ft-massive-texts-lab</td>\n","      <td>09-21-15-51-50</td>\n","    </tr>\n","    <tr>\n","      <th>gt-bypart3</th>\n","      <th>ada</th>\n","      <th>1.00</th>\n","      <td>ada:ft-massive-texts-lab:gt-bypart3-2022-07-22...</td>\n","      <td>ft-massive-texts-lab</td>\n","      <td>07-22-19-33-36</td>\n","    </tr>\n","    <tr>\n","      <th rowspan=\"3\" valign=\"top\">gt-byprompt</th>\n","      <th>ada</th>\n","      <th>1.00</th>\n","      <td>ada:ft-massive-texts-lab:gt-byprompt-2022-08-1...</td>\n","      <td>ft-massive-texts-lab</td>\n","      <td>08-14-02-20-49</td>\n","    </tr>\n","    <tr>\n","      <th>babbage</th>\n","      <th>1.00</th>\n","      <td>babbage:ft-massive-texts-lab:gt-byprompt-2022-...</td>\n","      <td>ft-massive-texts-lab</td>\n","      <td>08-13-21-01-44</td>\n","    </tr>\n","    <tr>\n","      <th>curie</th>\n","      <th>1.00</th>\n","      <td>curie:ft-massive-texts-lab:gt-byprompt-2022-08...</td>\n","      <td>ft-massive-texts-lab</td>\n","      <td>08-13-21-27-20</td>\n","    </tr>\n","    <tr>\n","      <th rowspan=\"20\" valign=\"top\">gt-main2</th>\n","      <th rowspan=\"8\" valign=\"top\">ada</th>\n","      <th>0.01</th>\n","      <td>ada:ft-massive-texts-lab:gt-main2-0-01-2022-08...</td>\n","      <td>ft-massive-texts-lab</td>\n","      <td>08-03-21-54-16</td>\n","    </tr>\n","    <tr>\n","      <th>0.05</th>\n","      <td>ada:ft-massive-texts-lab:gt-main2-0-05-2022-08...</td>\n","      <td>ft-massive-texts-lab</td>\n","      <td>08-03-22-02-14</td>\n","    </tr>\n","    <tr>\n","      <th>0.10</th>\n","      <td>ada:ft-massive-texts-lab:gt-main2-0-1-2022-08-...</td>\n","      <td>ft-massive-texts-lab</td>\n","      <td>08-03-22-21-38</td>\n","    </tr>\n","    <tr>\n","      <th>0.20</th>\n","      <td>ada:ft-massive-texts-lab:gt-main2-0-2-2022-08-...</td>\n","      <td>ft-massive-texts-lab</td>\n","      <td>08-03-22-43-02</td>\n","    </tr>\n","    <tr>\n","      <th>0.40</th>\n","      <td>ada:ft-massive-texts-lab:gt-main2-0-4-2022-08-...</td>\n","      <td>ft-massive-texts-lab</td>\n","      <td>08-03-23-03-18</td>\n","    </tr>\n","    <tr>\n","      <th>0.60</th>\n","      <td>ada:ft-massive-texts-lab:gt-main2-0-6-2022-08-...</td>\n","      <td>ft-massive-texts-lab</td>\n","      <td>08-03-23-18-39</td>\n","    </tr>\n","    <tr>\n","      <th>0.80</th>\n","      <td>ada:ft-massive-texts-lab:gt-main2-0-8-2022-08-...</td>\n","      <td>ft-massive-texts-lab</td>\n","      <td>08-03-23-42-25</td>\n","    </tr>\n","    <tr>\n","      <th>1.00</th>\n","      <td>ada:ft-massive-texts-lab:gt-main2-2022-08-01-1...</td>\n","      <td>ft-massive-texts-lab</td>\n","      <td>08-01-19-24-54</td>\n","    </tr>\n","    <tr>\n","      <th rowspan=\"8\" valign=\"top\">babbage</th>\n","      <th>0.01</th>\n","      <td>babbage:ft-massive-texts-lab:gt-main2-0-01-202...</td>\n","      <td>ft-massive-texts-lab</td>\n","      <td>08-03-21-10-14</td>\n","    </tr>\n","    <tr>\n","      <th>0.05</th>\n","      <td>babbage:ft-massive-texts-lab:gt-main2-0-05-202...</td>\n","      <td>ft-massive-texts-lab</td>\n","      <td>08-02-00-44-18</td>\n","    </tr>\n","    <tr>\n","      <th>0.10</th>\n","      <td>babbage:ft-massive-texts-lab:gt-main2-0-1-2022...</td>\n","      <td>ft-massive-texts-lab</td>\n","      <td>08-02-01-09-21</td>\n","    </tr>\n","    <tr>\n","      <th>0.20</th>\n","      <td>babbage:ft-massive-texts-lab:gt-main2-0-2-2022...</td>\n","      <td>ft-massive-texts-lab</td>\n","      <td>08-01-20-28-00</td>\n","    </tr>\n","    <tr>\n","      <th>0.40</th>\n","      <td>babbage:ft-massive-texts-lab:gt-main2-0-4-2022...</td>\n","      <td>ft-massive-texts-lab</td>\n","      <td>08-01-20-52-49</td>\n","    </tr>\n","    <tr>\n","      <th>0.60</th>\n","      <td>babbage:ft-massive-texts-lab:gt-main2-0-6-2022...</td>\n","      <td>ft-massive-texts-lab</td>\n","      <td>08-01-21-10-41</td>\n","    </tr>\n","    <tr>\n","      <th>0.80</th>\n","      <td>babbage:ft-massive-texts-lab:gt-main2-0-8-2022...</td>\n","      <td>ft-massive-texts-lab</td>\n","      <td>08-01-21-32-17</td>\n","    </tr>\n","    <tr>\n","      <th>1.00</th>\n","      <td>babbage:ft-massive-texts-lab:gt-main2-2022-08-...</td>\n","      <td>ft-massive-texts-lab</td>\n","      <td>08-01-19-26-25</td>\n","    </tr>\n","    <tr>\n","      <th rowspan=\"2\" valign=\"top\">curie</th>\n","      <th>0.01</th>\n","      <td>curie:ft-massive-texts-lab:gt-main2-0-01-2022-...</td>\n","      <td>ft-massive-texts-lab</td>\n","      <td>08-05-19-05-45</td>\n","    </tr>\n","    <tr>\n","      <th>1.00</th>\n","      <td>curie:ft-massive-texts-lab:gt-main2-2022-08-01...</td>\n","      <td>ft-massive-texts-lab</td>\n","      <td>08-01-19-44-29</td>\n","    </tr>\n","    <tr>\n","      <th rowspan=\"2\" valign=\"top\">davinci</th>\n","      <th>0.01</th>\n","      <td>davinci:ft-massive-texts-lab:gt-main2-0-01-202...</td>\n","      <td>ft-massive-texts-lab</td>\n","      <td>08-05-18-49-12</td>\n","    </tr>\n","    <tr>\n","      <th>1.00</th>\n","      <td>davinci:ft-massive-texts-lab:gt-main2-2022-08-...</td>\n","      <td>ft-massive-texts-lab</td>\n","      <td>08-05-16-46-47</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-211b269b-9d12-4ca4-b686-81957e2ea01a')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-211b269b-9d12-4ca4-b686-81957e2ea01a button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-211b269b-9d12-4ca4-b686-81957e2ea01a');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["                                                                                    name  \\\n","split              size    proportion                                                      \n","gt-alltests2-perm1 curie   1.00        curie:ft-massive-texts-lab:gt-alltests2-perm1-...   \n","gt-alltests2-perm2 curie   1.00        curie:ft-massive-texts-lab:gt-alltests2-perm2-...   \n","gt-alltests2-perm3 curie   1.00        curie:ft-massive-texts-lab:gt-alltests2-perm3-...   \n","gt-bypart3         ada     1.00        ada:ft-massive-texts-lab:gt-bypart3-2022-07-22...   \n","gt-byprompt        ada     1.00        ada:ft-massive-texts-lab:gt-byprompt-2022-08-1...   \n","                   babbage 1.00        babbage:ft-massive-texts-lab:gt-byprompt-2022-...   \n","                   curie   1.00        curie:ft-massive-texts-lab:gt-byprompt-2022-08...   \n","gt-main2           ada     0.01        ada:ft-massive-texts-lab:gt-main2-0-01-2022-08...   \n","                           0.05        ada:ft-massive-texts-lab:gt-main2-0-05-2022-08...   \n","                           0.10        ada:ft-massive-texts-lab:gt-main2-0-1-2022-08-...   \n","                           0.20        ada:ft-massive-texts-lab:gt-main2-0-2-2022-08-...   \n","                           0.40        ada:ft-massive-texts-lab:gt-main2-0-4-2022-08-...   \n","                           0.60        ada:ft-massive-texts-lab:gt-main2-0-6-2022-08-...   \n","                           0.80        ada:ft-massive-texts-lab:gt-main2-0-8-2022-08-...   \n","                           1.00        ada:ft-massive-texts-lab:gt-main2-2022-08-01-1...   \n","                   babbage 0.01        babbage:ft-massive-texts-lab:gt-main2-0-01-202...   \n","                           0.05        babbage:ft-massive-texts-lab:gt-main2-0-05-202...   \n","                           0.10        babbage:ft-massive-texts-lab:gt-main2-0-1-2022...   \n","                           0.20        babbage:ft-massive-texts-lab:gt-main2-0-2-2022...   \n","                           0.40        babbage:ft-massive-texts-lab:gt-main2-0-4-2022...   \n","                           0.60        babbage:ft-massive-texts-lab:gt-main2-0-6-2022...   \n","                           0.80        babbage:ft-massive-texts-lab:gt-main2-0-8-2022...   \n","                           1.00        babbage:ft-massive-texts-lab:gt-main2-2022-08-...   \n","                   curie   0.01        curie:ft-massive-texts-lab:gt-main2-0-01-2022-...   \n","                           1.00        curie:ft-massive-texts-lab:gt-main2-2022-08-01...   \n","                   davinci 0.01        davinci:ft-massive-texts-lab:gt-main2-0-01-202...   \n","                           1.00        davinci:ft-massive-texts-lab:gt-main2-2022-08-...   \n","\n","                                                        lab            date  \n","split              size    proportion                                        \n","gt-alltests2-perm1 curie   1.00        ft-massive-texts-lab  09-20-23-08-34  \n","gt-alltests2-perm2 curie   1.00        ft-massive-texts-lab  09-21-15-22-40  \n","gt-alltests2-perm3 curie   1.00        ft-massive-texts-lab  09-21-15-51-50  \n","gt-bypart3         ada     1.00        ft-massive-texts-lab  07-22-19-33-36  \n","gt-byprompt        ada     1.00        ft-massive-texts-lab  08-14-02-20-49  \n","                   babbage 1.00        ft-massive-texts-lab  08-13-21-01-44  \n","                   curie   1.00        ft-massive-texts-lab  08-13-21-27-20  \n","gt-main2           ada     0.01        ft-massive-texts-lab  08-03-21-54-16  \n","                           0.05        ft-massive-texts-lab  08-03-22-02-14  \n","                           0.10        ft-massive-texts-lab  08-03-22-21-38  \n","                           0.20        ft-massive-texts-lab  08-03-22-43-02  \n","                           0.40        ft-massive-texts-lab  08-03-23-03-18  \n","                           0.60        ft-massive-texts-lab  08-03-23-18-39  \n","                           0.80        ft-massive-texts-lab  08-03-23-42-25  \n","                           1.00        ft-massive-texts-lab  08-01-19-24-54  \n","                   babbage 0.01        ft-massive-texts-lab  08-03-21-10-14  \n","                           0.05        ft-massive-texts-lab  08-02-00-44-18  \n","                           0.10        ft-massive-texts-lab  08-02-01-09-21  \n","                           0.20        ft-massive-texts-lab  08-01-20-28-00  \n","                           0.40        ft-massive-texts-lab  08-01-20-52-49  \n","                           0.60        ft-massive-texts-lab  08-01-21-10-41  \n","                           0.80        ft-massive-texts-lab  08-01-21-32-17  \n","                           1.00        ft-massive-texts-lab  08-01-19-26-25  \n","                   curie   0.01        ft-massive-texts-lab  08-05-19-05-45  \n","                           1.00        ft-massive-texts-lab  08-01-19-44-29  \n","                   davinci 0.01        ft-massive-texts-lab  08-05-18-49-12  \n","                           1.00        ft-massive-texts-lab  08-05-16-46-47  "]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["#@markdown Show all fine-tuned models\n","rows = []\n","all_models = [(x['id']) for x in openai.Model.list()['data'] if 'massive-texts-lab' in x['id']]\n","# or see all fine tunes (including deleted models)\n","#all_models = [(x['fine_tuned_model']) for x in openai.FineTune.list()['data'] if x['status'] == 'succeeded']\n","for x in all_models:\n","    modelsize, lab, fullname = x.split(':')\n","    split, date = fullname.split('-2022-')\n","    if split.count('-') == 3:\n","        a,b,c,d = split.split('-')\n","        split, proportion = f\"{a}-{b}\", float(f\"{c}.{d}\")\n","    else:\n","        proportion = 1\n","    rows.append((x, modelsize, lab, split, proportion, date))\n","all_models = pd.DataFrame(rows, columns=['name', 'size', 'lab', 'split', 'proportion', 'date'])\n","all_models['proportion'] = all_models.proportion.astype(float)\n","all_models = all_models.set_index(['split', 'size', 'proportion']).sort_index()\n","all_models"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"OtAIWeQJy5ne"},"outputs":[],"source":["#@title delete old models\n","model_to_delete = \"curie:ft-massive-texts-lab:gt-alltests1-perm1-2022-09-20-19-42-57\" #@param {type: 'string'}\n","results = None\n","if model_to_delete:\n","    results = openai.Model.delete(model_to_delete)\n","results"]},{"cell_type":"code","execution_count":48,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1663778390186,"user":{"displayName":"Peter Organisciak","userId":"18310606572067872626"},"user_tz":360},"id":"o9jKwmAg_Vhx","outputId":"f31493a1-684e-4aa9-df16-1ccd56e0592b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using curie:ft-massive-texts-lab:gt-main2-2022-08-01-19-44-29\n"]}],"source":["#@title Select model.\n","finetuned_size = \"curie\" #@param [\"ada\", \"babbage\", \"curie\", \"davinci\"]\n","finetuned_split = \"gt-main2\" #@param [\"gt-byparticipant\", \"gt-byprompt\", \"gt-main2\"] {allow-input: true}\n","finetuned_suffix = \"\" #@param [\"\"] {allow-input: true}\n","finetuned_proportion = 1 #@param [0.01, 0.05, 0.1, 0.2, 0.4, 0.6, 0.8, 1.0] {type:\"raw\"}\n","\n","try:\n","    finetuned_model = all_models.loc[(finetuned_split+finetuned_suffix, finetuned_size, float(finetuned_proportion)), 'name']\n","    print(\"Using\", finetuned_model)\n","except KeyError:\n","    print(\"No trained model for those settings\")\n","    raise"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JtaCKldVg1lH"},"outputs":[],"source":["def score_gpt(prompt, model, just_final=False):\n","    response = openai.Completion.create(\n","        model=model,\n","        prompt=prompt,\n","        temperature=0,\n","        n=1,\n","        logprobs=None,\n","        stop='\\n',\n","        max_tokens=1\n","    )\n","    if just_final:\n","        return response.choices[0]['text']\n","    else:\n","        return response\n","\n","def score_batch(gptprompts, model, raise_errs=False, batch_size=500, **kwargs):\n","    ''' this is adapted from the batch scoring in the Open Scoring library module\n","    https://github.com/massivetexts/open-scoring/blob/master/open_scoring/scoring.py\n","    '''\n","    scores = []\n","    nbatches = np.ceil(len(gptprompts) / batch_size).astype(int)\n","    for i in tqdm(range(nbatches)):\n","        promptbatch = gptprompts[i*batch_size:(i+1)*batch_size]\n","\n","        sleeptime = 10\n","        while True:\n","            try:\n","                response = score_gpt(promptbatch, model=model, just_final=False)\n","                break\n","            except openai.error.RateLimitError:\n","                print(f\"Rate limit error - trying again in {sleeptime} seconds\")\n","                time.sleep(sleeptime)\n","                sleeptime += 2\n","        total_tokens = response.usage.total_tokens\n","        scores_raw = [x.text for x in response.choices]\n","        avg_tokens_per = total_tokens / len(scores_raw)\n","        for i, score_raw in enumerate(scores_raw):\n","            try:\n","                score = int(score_raw.strip()) / 10\n","            except:\n","                if raise_errs:\n","                    print(f\"GPT prompt: {promptbatch[i].strip()}\")\n","                    print(f\"raw response: {score_raw}\")\n","                    raise\n","                score = None\n","            scores.append((score, avg_tokens_per))\n","    return scores\n","\n","testdata = all_data[all_data.split.isin(setnames['test'])].copy()\n","results = score_batch(testdata.gptprompt.tolist(), finetuned_model, batch_size=400)\n","testdata[['predicted', 'total_tokens']] = pd.DataFrame(results).values\n","\n","s = finetuned_split.replace('-', '_')\n","\n","if (finetuned_proportion < 1) and (finetuned_suffix == \"\"):\n","    raise Exception(\"This condition not coded yet!\")\n","\n","if finetuned_suffix != \"\":\n","    fname = f'gpt-ft-{finetuned_size}-s{finetuned_suffix}.csv'\n","elif finetuned_proportion == 1:\n","    fname = f'gpt-ft-{finetuned_size}.csv'\n","else:\n","    fname  = f'gpt-ft-{finetuned_size}-{finetuned_proportion}.csv'\n","\n","testdata['model'] = f\"gpt3-{finetuned_size}\"\n","testdata['proportion'] = finetuned_proportion\n","\n","#testdata['predicted'] = testdata.predicted_raw.str.strip().str.replace('[\\-\\:/]','', regex=True).apply(lambda x:x.split(' ')[0])\n","#testdata['predicted'] = pd.to_numeric(testdata['predicted'], errors='coerce').div(10)\n","returncols = ['id', 'model', 'type', 'participant', 'prompt', 'target', 'predicted', 'src', 'total_tokens', 'proportion']\n","output = testdata[[x for x in returncols if x in testdata.columns]]\n","print(\"Saving to\", (base_dir / 'Data' / 'evaluation' / s / fname))\n","output.to_csv(base_dir / 'Data' / 'evaluation' / s / fname)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":304,"status":"ok","timestamp":1663705135598,"user":{"displayName":"Peter Organisciak","userId":"18310606572067872626"},"user_tz":360},"id":"JErSUXh2gsT7","outputId":"b6c67239-624d-4ccc-8e7b-e8da2aa8588f"},"outputs":[{"data":{"text/plain":["type                   \n","completion    predicted    0.847867\n","consequences  predicted    0.713104\n","instances     predicted    0.907785\n","uses          predicted    0.785307\n","Name: target, dtype: float64"]},"execution_count":122,"metadata":{},"output_type":"execute_result"}],"source":["output.groupby('type')[['target', 'predicted']].corr().iloc[1::2, 0]"]},{"cell_type":"markdown","metadata":{"id":"mozRwXNdZgDp"},"source":["## Special Eval\n","\n","This code is specific for the multiple permutation training (where multiple models were trained on different data, to allow scores for 100% of the data without any data leakage). You probably don't need this :)"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":269,"status":"ok","timestamp":1663776777537,"user":{"displayName":"Peter Organisciak","userId":"18310606572067872626"},"user_tz":360},"id":"bAUYGj8u0lm9","outputId":"82734310-89ec-49c6-c036-a73f8d208e22"},"outputs":[{"data":{"text/plain":["['curie:ft-massive-texts-lab:gt-alltests2-perm1-2022-09-20-23-08-34',\n"," 'curie:ft-massive-texts-lab:gt-alltests2-perm2-2022-09-21-15-22-40',\n"," 'curie:ft-massive-texts-lab:gt-alltests2-perm3-2022-09-21-15-51-50']"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["all_models[all_models.name.str.contains('alltests2')].name.tolist()"]},{"cell_type":"code","execution_count":56,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":289,"status":"ok","timestamp":1663779078109,"user":{"displayName":"Peter Organisciak","userId":"18310606572067872626"},"user_tz":360},"id":"vqBXnot8BlVB","outputId":"fb941006-f08c-448f-dedb-08dbd7e589cf"},"outputs":[{"data":{"text/plain":["type                 \n","completion  predicted    0.851884\n","instances   predicted    0.787339\n","uses        predicted    0.747670\n","Name: target, dtype: float64"]},"execution_count":56,"metadata":{},"output_type":"execute_result"}],"source":["modelname = ''\n","all_eval = []\n","for gptmodel, testpath in [('curie:ft-massive-texts-lab:gt-alltests2-perm1-2022-09-20-23-08-34', 'group3'),\n","                            ('curie:ft-massive-texts-lab:gt-alltests2-perm2-2022-09-21-15-22-40', 'group2'),\n","                            ('curie:ft-massive-texts-lab:gt-alltests2-perm3-2022-09-21-15-51-50', 'group1'),\n","                            ('curie:ft-massive-texts-lab:gt-alltests2-perm3-2022-09-21-15-51-50', 'val')]:\n","    df = pd.DataFrame([pd.read_json(x, orient='index')[0] for x in (data_dir / testpath).glob('*json')])\n","    just_motes = df.query('src==\"motesf\"')\n","    just_motes = gt_preparation(just_motes)\n","    results = score_batch(just_motes.gptprompt.tolist(), gptmodel, batch_size=600)\n","    just_motes[['predicted', 'total_tokens']] = pd.DataFrame(results).values\n","    all_eval.append(just_motes)\n","    \n","fulldata = pd.concat(all_eval)\n","fulldata['participant'] = fulldata['participant'].str.replace('motesf','')\n","fulldata.prompt = fulldata.prompt.str.replace('lightbulb', 'light bulbs').str.replace('hat', 'hat cap').str.replace('ball','soccer ball').str.replace('pencil', 'lead pencil').str.replace('spoon', 'spoons')\n","fulldata.to_csv(base_dir / 'motesf_alltest2_allperms.csv')\n","fulldata.groupby('type')[['target', 'predicted']].corr().iloc[1::2, 0]"]},{"cell_type":"code","execution_count":54,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":81},"executionInfo":{"elapsed":498,"status":"ok","timestamp":1663778986311,"user":{"displayName":"Peter Organisciak","userId":"18310606572067872626"},"user_tz":360},"id":"s45F0Vyz2HCO","outputId":"89f94f5e-0479-49c8-c6c0-04a8f88ce2f8"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-03508da5-3399-49c6-8959-50fa0d62a823\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Order</th>\n","      <th>response_num</th>\n","      <th>prompt</th>\n","      <th>response</th>\n","      <th>target</th>\n","      <th>predicted</th>\n","      <th>type</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>7540</th>\n","      <td>126</td>\n","      <td>6</td>\n","      <td>rain</td>\n","      <td>cat dog</td>\n","      <td>2.3</td>\n","      <td>1.9</td>\n","      <td>completion</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-03508da5-3399-49c6-8959-50fa0d62a823')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-03508da5-3399-49c6-8959-50fa0d62a823 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-03508da5-3399-49c6-8959-50fa0d62a823');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["      Order response_num prompt response  target  predicted        type\n","7540    126            6   rain  cat dog     2.3        1.9  completion"]},"execution_count":54,"metadata":{},"output_type":"execute_result"}],"source":["# Merge back to original data\n","import hashlib\n","df = pd.read_csv(gt_dir / 'motes_full_gt_scores.csv').replace(-999, pd.NA).copy()\n","items = [col.replace('_prompt', '') for col in df.columns if col.startswith('G') and col.endswith('_prompt')]\n","collector = []\n","for item in items:\n","    subset = df[['Order'] + [col for col in df.columns if col.startswith(item)]].copy()\n","    subset.columns = [col.split('_')[-1] for col in subset.columns]\n","    subset['game'] = item.split('_')[0]\n","    subset['prompt_code'] = item\n","    collector.append(subset)\n","reshaped = pd.concat(collector)\n","reshaped = reshaped.rename(columns={'corrected':'response'})\n","final = reshaped.merge(fulldata.drop('participant', axis=1), on=['prompt', 'response'])[['Order', 'response_num', 'prompt', 'response', 'target', 'predicted', 'type']]\n","final.to_csv(base_dir / 'motesf-llm-scores.csv')\n","final.sample(1)"]},{"cell_type":"markdown","metadata":{"id":"xIMQRzjIgCi0"},"source":["# 3. Prompt Engineering (Zero-shot or Few-shot)"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"bXlmOpn-Nr2E"},"outputs":[],"source":["base_dir = Path('drive/MyDrive/Grants/MOTES/') #@param { type: 'raw' }\n","data_subdir = \"gt_main_std\" # should be identical to gt_main2, but I didn't want to accidentally overwrite gt_main when exporting standard deviations\n","!cp \"{gt_dir}/{data_subdir}.tar.gz\" .\n","!rm -rf data\n","!tar -xf {data_subdir}.tar.gz\n","data_dir = Path(f\"data/{data_subdir}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"whmcimlmPi47"},"outputs":[],"source":["testdata = pd.DataFrame([pd.read_json(x, orient='index')[0] for x in (data_dir / 'test').glob('*json')])\n","traindata = pd.DataFrame([pd.read_json(x, orient='index')[0] for x in (data_dir / 'train').glob('*json')])"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"hh666Js4lebK"},"outputs":[],"source":["#@title Few/Zero Shot Definitions\n","import re\n","PROMPT_TEMPLATE = \"Below is a list of uses for a {0}. On a scale of 10-50, judge how original each use for a {1} is, where 10 is 'not at all creative' and 50 is 'very creative':\\n\\nUSES\\n{2}\\n{3}\\n\\nRATINGS\\n{4}\\n{5}\" #@param {type:\"raw\"}\n","#@markdown Examples and completions per prompt. nexamples=0 for zero-shot. Note that this may not work.\n","nexamples = 5 #@param {type:\"integer\"}\n","ncompletions =  5#@param {type:\"integer\"}\n","\n","def select_by_std(x, nexamples=5, max_std=None):\n","    ''' Choose examples from low stdev group that span the range of '''\n","    z = x.sort_values('target')\n","    if max_std:\n","        z = z[z.rating_std <= max_std]\n","    nexamples = min(len(z), nexamples)\n","    batch_size = len(z) // nexamples\n","    samples = []\n","    for i in range(nexamples): \n","        maxv = (i+1)*batch_size if (i+1) < nexamples else len(z)+1\n","        sample = z.iloc[(i*batch_size):maxv].sample(1).iloc[0]\n","        samples.append(sample)\n","    return pd.DataFrame(samples)\n","\n","def format_fewshot(df, startn=1, no_target=False, shuffle=True):\n","    assert len(df.question.unique()) == 1\n","    if shuffle:\n","        # jumble, so GPT doesn't speculate based on monotonically increasing values\n","        df = df.sample(frac=1)\n","    q = df.iloc[0]['question']\n","    rlist = \"\\n\".join([f\"{i+startn}. {response}\" for i, response in enumerate(df.response)])\n","    if no_target:\n","        tlist = f\"{startn}.\"\n","    else:\n","        tlist = \"\\n\".join([f\"{i+startn}. {int(target*10)}\" for i, target in enumerate(df.target)])\n","    return pd.Series(dict(q=q, rlist=rlist, tlist=tlist))\n","\n","def fewshot_prompt(x, nexamples=5, max_std=None):\n","    df = select_by_std(x, nexamples=nexamples, max_std=max_std)\n","    return format_fewshot(df)\n","\n","def chunked_prompts(x, startn=6, ncomplete = 5, no_target=True):\n","    nchunks = np.ceil(len(x) / ncomplete).astype(int)\n","    rows = []\n","    for i in range(nchunks):\n","        subset = x[i::nchunks]\n","        rows.append(format_fewshot(subset, startn=startn, no_target=no_target))\n","    return pd.DataFrame(rows)\n","\n","def parse_uses_response(gptresponse, startn = 6, ncomplete = 5):\n","    prompt = re.findall(\" each use for a? ?(.*?) is, where \\d+ is '\", gptresponse)[0]\n","    raw_list = re.findall('(\\d+\\..*)$', gptresponse, flags=re.MULTILINE)\n","    cleaned = []\n","    for n in range(startn, startn+ncomplete):\n","        rawvals = [x for x in raw_list if x.startswith(f\"{n}. \")]\n","        vals = [x.split('.', 1)[-1].strip() for x in rawvals]\n","        if len(vals) > 2:\n","            print(f\"ERROR WITH {n}: {vals}\")\n","        elif len(vals) == 0:\n","            # skip\n","            continue\n","\n","        try:\n","            response, predicted = vals[0], vals[1]\n","            if not predicted.isnumeric():\n","                # failsafe where everything but last two integers are stripped\n","                # many ways this can fail, but let it fail\n","                predicted = \"\".join([x for x in list(predicted) if x.isnumeric()][-2:])\n","        except:\n","            print(f\"Can't Parse {n}: {rawvals}\")\n","\n","        try:\n","            predicted = int(predicted)\n","            cleaned.append(dict(prompt=prompt, response=response, predicted=predicted))\n","        except:\n","            print(f\"Can't cast predicted to int {n}: {rawvals}\")\n","    return pd.DataFrame(cleaned)\n","\n","def full_prompt_from_completes_row(row):\n","    start = prompt_starts.loc[row.name]\n","    full_prompt = PROMPT_TEMPLATE.format(row.name.upper(), row.name.lower(),start.rlist, row.rlist,start.tlist, row.tlist)\n","    full_prompt = full_prompt.replace('A PANTS', 'PANTS').replace('a pants', 'pants')\n","    return full_prompt"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1272,"status":"ok","timestamp":1659555944864,"user":{"displayName":"Peter Organisciak","userId":"18310606572067872626"},"user_tz":360},"id":"OC_pbRSXP0Q1","outputId":"a6a9a572-3276-41ed-cc6b-e1b5418524e9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Below is a list of uses for a SOCK. On a scale of 10-50, judge how original each use for a sock is, where 10 is 'not at all creative' and 50 is 'very creative':\n","\n","USES\n","1. to use it like a puppet.\n","2. You can put googly eyes and make a sock puppet show.\n","3. You can color it and maybe make a snake.\n","4. a cool and funny puppet.\n","5. maybe you could put it on your hands and pretend to have superpowers.\n","6. using it as gloves.\n","7. you could use it for ASMR\n","8. Cut them and make a 3D scupture.\n","9. you can make a dress for your doll\n","10. to use it like a backpack or store money in it\n","\n","RATINGS\n","1. 27\n","2. 27\n","3. 32\n","4. 24\n","5. 36\n","6.\n"]}],"source":["sdlim = traindata.rating_std.quantile(0.4)\n","\n","prompt_completes = testdata.groupby('prompt').apply(lambda x: chunked_prompts(x, startn=nexamples+1, ncomplete=ncompletions)).reset_index(1)\n","if nexamples > 0:\n","    prompt_starts = traindata.groupby(['prompt']).apply(lambda x: fewshot_prompt(x, nexamples=nexamples, max_std=sdlim))\n","else:\n","    prompt_starts = traindata[['prompt', 'question']].drop_duplicates().set_index('prompt').rename(columns={'question':'q'})\n","    prompt_starts[['rlist','tlist']] = ('', '')\n","\n","prompt_completes['prompt_for_gpt'] = prompt_completes.apply(full_prompt_from_completes_row, axis=1)\n","\n","if nexamples ==  0:\n","    def replaces(x):\n","        return (x.replace('USES\\n\\n', 'USES\\n')\n","        .replace('RATINGS\\n\\n', 'RATING FROM 1-10\\n')\n","        .replace('10-50', '1-10').replace('10 is', '1 is').replace('50 is', '10 is') # change scale to be 1-10 (/2 for final score)\n","        )\n","    prompt_completes['prompt_for_gpt'] = prompt_completes['prompt_for_gpt'].apply(replaces, 1)\n","\n","testprompt = prompt_completes.sample().iloc[0]['prompt_for_gpt']\n","print(testprompt)"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"I-aZlu4uk5JP"},"outputs":[],"source":["fewshot_model = \"text-davinci-002\" #@param ['text-ada-001', 'text-babbage-001', 'text-curie-001', 'text-davinci-002']\n","\n","def score_fewshot(prompt, model=fewshot_model, just_final=False):\n","    response = openai.Completion.create(\n","        model=model,\n","        prompt=prompt,\n","        temperature=0,\n","        n=1,\n","        logprobs=None,\n","        max_tokens=200\n","    )\n","    if just_final:\n","        return response.choices[0]['text']\n","    else:\n","        return response"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2462,"status":"ok","timestamp":1659730676287,"user":{"displayName":"Peter Organisciak","userId":"18310606572067872626"},"user_tz":360},"id":"OEYD3wDfp1v6","outputId":"63b0dffa-9c9e-4b60-8034-052bab1feed9"},"outputs":[{"name":"stdout","output_type":"stream","text":[" 20\n","7. 40\n","8. 50\n","9. 45\n","10. 35\n"]}],"source":["# example in paper\n","testprompt = '''Below is a list of uses for a SOCK. On a scale of 10-50, judge how original each use for a sock is, where 10 is 'not at all creative' and 50 is 'very creative':\n","\n","USES\n","1. to use it like a puppet.\n","2. You can put googly eyes and make a sock puppet show.\n","3. You can color it and maybe make a snake.\n","4. a cool and funny puppet.\n","5. maybe you could put it on your hands and pretend to have superpowers.\n","6. using it as gloves.\n","7. you could use it for ASMR\n","8. Cut them and make a 3D scupture.\n","9. you can make a dress for your doll\n","10. to use it like a backpack or store money in it\n","\n","RATINGS\n","1. 27\n","2. 27\n","3. 32\n","4. 24\n","5. 36\n","6.'''\n","gptresponse = score_fewshot(testprompt)\n","print(gptresponse['choices'][0]['text'])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","referenced_widgets":["84aba033405b4d99a4d7e4c60ef7f588","dc0da3453b29407a82f0afe45cf33a36","b9991220103e4929ac2235924417e9ab","889f26807e7c4e2eba4200e56a49fbe8","33bfed350c39433c9d4a1bed64b03a0a","4a71f25037c248ed9a1fe7622313b58e","d6338298e86a4d0ba225e3526e8beab2","642ee338a2d748ca89fcfbf20fa6cc46","357345817ea04014a672717ae966b6a7","623ee6bf5f9e43cbb2ad080418046d93","e2a4f90a6e3e49878d42b331466de28b"]},"id":"Pf_IAXRTvrb_","outputId":"9f5cb7d2-6fe5-4b12-c49f-8b014d300dd5"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"84aba033405b4d99a4d7e4c60ef7f588","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/615 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["collector = []\n","\n","davinci_cost = 0.06/1000\n","total_usage = 0\n","n_processed = 0\n","err_prompts = []\n","\n","pbar = tqdm(prompt_completes.prompt_for_gpt)\n","for fullprompt in pbar:\n","    try:\n","        gptresponse = score_fewshot(fullprompt)\n","        completed = (fullprompt + gptresponse['choices'][0]['text'])\n","        out = parse_uses_response(completed, startn=nexamples+1, ncomplete=ncompletions)\n","        out['usage_per_prompt'] = gptresponse['usage']['total_tokens'] / 10\n","        collector.append(out)\n","        # Calculate costs\n","        n_processed += len(out)\n","        total_usage += gptresponse['usage']['total_tokens']\n","        est = np.round(len(testdata) * (total_usage / n_processed) * davinci_cost, 2)\n","        pbar.set_description(f\"Cost est (davinci). $%s\" % est)\n","    except KeyboardInterrupt:\n","        raise\n","    except:\n","        raise\n","        print(\"Err with \", completed)\n","        err_prompts.append((fullprompt, gptresponse['choices'][0]['text']))\n","\n","all_fewshot = pd.concat(collector)\n","all_fewshot"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":142,"status":"ok","timestamp":1659557721590,"user":{"displayName":"Peter Organisciak","userId":"18310606572067872626"},"user_tz":360},"id":"dCAbwe8HWKHU","outputId":"b0b03eeb-1662-463a-d806-eeebc279e92c"},"outputs":[{"data":{"text/plain":["3.1992419999999995"]},"execution_count":87,"metadata":{},"output_type":"execute_result"}],"source":["all_fewshot.usage_per_prompt.sum() * davinci_cost"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":146,"status":"ok","timestamp":1659557797023,"user":{"displayName":"Peter Organisciak","userId":"18310606572067872626"},"user_tz":360},"id":"niOwmeZZWjv6","outputId":"b9435619-5d29-4837-84ef-661bd6a2faa7"},"outputs":[{"data":{"text/plain":["src             3030\n","question        3030\n","prompt          3030\n","response        3030\n","id              3030\n","target          3030\n","participant     3030\n","response_num    2249\n","rating_std      3023\n","count            338\n","dtype: int64"]},"execution_count":91,"metadata":{},"output_type":"execute_result"}],"source":["testdata.count()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":144,"status":"ok","timestamp":1659557782504,"user":{"displayName":"Peter Organisciak","userId":"18310606572067872626"},"user_tz":360},"id":"BSTCiKAbWXJq","outputId":"2a7d54f5-5b34-44f2-fb1c-ef1828cf3455"},"outputs":[{"data":{"text/plain":["2995"]},"execution_count":90,"metadata":{},"output_type":"execute_result"}],"source":["results.predicted.count()"]},{"cell_type":"markdown","metadata":{"id":"o9ilDINS6XFs"},"source":["Cost for DaVinci with 10 completions:\n"," - $4.32 (`r=0.42`, n(errs)=39)\n","\n"," Cost for DaVinci with 5 completions:\n"," - $ 3.20 (`r=0.36`, n(errs)=36) (huh? Why lower?)\n","\n","Cost for DaVinci zero-shot with 10 completions:\n","- $3.34 (`r=0.13`, n(errs)=106)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":164,"status":"ok","timestamp":1659557636216,"user":{"displayName":"Peter Organisciak","userId":"18310606572067872626"},"user_tz":360},"id":"48aGsl6a3vir","outputId":"d7eabe66-4351-438f-f46f-7528935359fc"},"outputs":[{"name":"stdout","output_type":"stream","text":["n(errs)=36\n"]}],"source":["results = testdata.merge(all_fewshot, on=['prompt', 'response'], how='left')\n","if nexamples > 0:\n","    results.predicted = results.predicted.div(10)\n","else:\n","    results.predicted = results.predicted.div(2)\n","results['model'] = f\"gpt3-{fewshot_model.split('-')[1]}\"\n","results['nexamples'] = nexamples\n","results['ncompletions'] = ncompletions\n","print(f\"n(errs)={results.predicted.isna().sum()}\")\n","results = results.rename(columns={'usage_per_prompt': 'total_tokens'})\n","\n","output = results[['id', 'model', 'participant', 'prompt', 'target', 'predicted', 'src', 'total_tokens', 'nexamples', 'ncompletions']]\n","(base_dir / 'Data' / 'evaluation' / 'fewshot').mkdir(exist_ok=True)\n","fname = f'gpt3-{fewshot_model}-{nexamples}-{ncompletions}.csv'\n","output.to_csv(base_dir / 'Data' / 'evaluation' / 'fewshot' / fname)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":151,"status":"ok","timestamp":1659557640014,"user":{"displayName":"Peter Organisciak","userId":"18310606572067872626"},"user_tz":360},"id":"fownuCa76iPk","outputId":"e8387180-9d21-4b51-fbb1-fc72b9830096"},"outputs":[{"data":{"text/plain":["0.3639529389033076"]},"execution_count":83,"metadata":{},"output_type":"execute_result"}],"source":["output.corr().loc['target', 'predicted']"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":124,"status":"ok","timestamp":1659555581892,"user":{"displayName":"Peter Organisciak","userId":"18310606572067872626"},"user_tz":360},"id":"13TcHX345LVR","outputId":"861a839d-194c-4545-91cb-5c51115fabba"},"outputs":[{"data":{"text/plain":["prompt            \n","backpack    target   -0.372925\n","ball        target    0.248275\n","book        target    0.009457\n","bottle      target    0.227964\n","box         target    0.223513\n","brick       target    0.103470\n","fork        target    0.271246\n","hat         target    0.430603\n","knife       target    0.118874\n","lightbulb   target    0.182097\n","pants       target    0.536770\n","paperclip   target    0.293652\n","pencil      target    0.213501\n","rope        target    0.210758\n","shoe        target    0.161375\n","shovel      target    0.002604\n","sock        target    0.078571\n","spoon       target    0.194876\n","table       target    0.055694\n","tire        target    0.126105\n","toothbrush  target    0.274063\n","Name: predicted, dtype: float64"]},"execution_count":69,"metadata":{},"output_type":"execute_result"}],"source":["output.groupby('prompt').corr().loc[(slice(None),'target'), 'predicted']"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyN8JlDaTJe5pcA9EwJp17jG","collapsed_sections":["EBRg4Kyxf_J5","r0aJav0JXR6V","mO8IRVw-wHac","BZ_8JgZY_Ksv","IcOEf0nF1NwV","mozRwXNdZgDp","xIMQRzjIgCi0"],"mount_file_id":"1r6CsSD6rEKp5WHjiSvIC4en8URISg42p","provenance":[]},"kernelspec":{"display_name":"Python 3.9.12 ('base')","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.9.12"},"vscode":{"interpreter":{"hash":"e93f6973baf80d840d9d6715f091ca11c0bba5902c2f522aa0ee562f8ebeaf67"}},"widgets":{"application/vnd.jupyter.widget-state+json":{"088c8aa40bb04668ab1c22c7d397484d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1c87b8ddd3ee46b5a6ad452bb7e9c5b6","placeholder":"​","style":"IPY_MODEL_ce2fc29454ac4c4aa23120f50adbf96a","value":"100%"}},"10877f4be0fe4c1ea04700363b97affe":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1c87b8ddd3ee46b5a6ad452bb7e9c5b6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2204af6101f345f291afaed89dc0b0a6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"22a33af88e794997917bfeabd9268358":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_57b78239874441b0bde0a1266675be92","max":21,"min":0,"orientation":"horizontal","style":"IPY_MODEL_41eea4226fd449538f2e8ab7f75ee6ec","value":21}},"325b1133d7e74183a7f998ead3e5a5ca":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"33bfed350c39433c9d4a1bed64b03a0a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"357345817ea04014a672717ae966b6a7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"41eea4226fd449538f2e8ab7f75ee6ec":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4a71f25037c248ed9a1fe7622313b58e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"57b78239874441b0bde0a1266675be92":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"623ee6bf5f9e43cbb2ad080418046d93":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"642ee338a2d748ca89fcfbf20fa6cc46":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6fc8c4bc63564a68830df17e92adc5fe":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_10877f4be0fe4c1ea04700363b97affe","max":3030,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e3e493cc1289468b9c9d030cf113748c","value":189}},"7597dc5105b84d9cb54d7093660e1b65":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"83fb33a63c29406ca99ea77bb8f4852d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"84aba033405b4d99a4d7e4c60ef7f588":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_dc0da3453b29407a82f0afe45cf33a36","IPY_MODEL_b9991220103e4929ac2235924417e9ab","IPY_MODEL_889f26807e7c4e2eba4200e56a49fbe8"],"layout":"IPY_MODEL_33bfed350c39433c9d4a1bed64b03a0a"}},"889f26807e7c4e2eba4200e56a49fbe8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_623ee6bf5f9e43cbb2ad080418046d93","placeholder":"​","style":"IPY_MODEL_e2a4f90a6e3e49878d42b331466de28b","value":" 430/615 [10:13&lt;04:22,  1.42s/it]"}},"9bccef81c5b84b9fbd2986d8ddd1b5d5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9edded9844eb4898a712fa606a6007b9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b9991220103e4929ac2235924417e9ab":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_642ee338a2d748ca89fcfbf20fa6cc46","max":615,"min":0,"orientation":"horizontal","style":"IPY_MODEL_357345817ea04014a672717ae966b6a7","value":430}},"ba0385ef83b24e379b3b8f604285ca02":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ea7edde4398f4f68aec4afc49192e432","placeholder":"​","style":"IPY_MODEL_325b1133d7e74183a7f998ead3e5a5ca","value":" 21/21 [00:00&lt;00:00, 107.12it/s]"}},"ba2083e30d24486f8261c28b95ae302e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c94a87cc40e443089b47cc14ccd590db":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_088c8aa40bb04668ab1c22c7d397484d","IPY_MODEL_22a33af88e794997917bfeabd9268358","IPY_MODEL_ba0385ef83b24e379b3b8f604285ca02"],"layout":"IPY_MODEL_9bccef81c5b84b9fbd2986d8ddd1b5d5"}},"ce2fc29454ac4c4aa23120f50adbf96a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d6338298e86a4d0ba225e3526e8beab2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dc0da3453b29407a82f0afe45cf33a36":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4a71f25037c248ed9a1fe7622313b58e","placeholder":"​","style":"IPY_MODEL_d6338298e86a4d0ba225e3526e8beab2","value":"Cost est (davinci). $6.47:  70%"}},"dd2d24cdf84a46948f541f1bfd3936d2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e7e28c4dd54349ae8bbdb168f1731560","IPY_MODEL_6fc8c4bc63564a68830df17e92adc5fe","IPY_MODEL_e24031fa5df146628d07ed285c84572d"],"layout":"IPY_MODEL_83fb33a63c29406ca99ea77bb8f4852d"}},"e24031fa5df146628d07ed285c84572d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ba2083e30d24486f8261c28b95ae302e","placeholder":"​","style":"IPY_MODEL_7597dc5105b84d9cb54d7093660e1b65","value":" 189/3030 [00:23&lt;07:34,  6.25it/s]"}},"e2a4f90a6e3e49878d42b331466de28b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e3e493cc1289468b9c9d030cf113748c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e7e28c4dd54349ae8bbdb168f1731560":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9edded9844eb4898a712fa606a6007b9","placeholder":"​","style":"IPY_MODEL_2204af6101f345f291afaed89dc0b0a6","value":"  6%"}},"ea7edde4398f4f68aec4afc49192e432":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}
